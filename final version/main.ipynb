{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbbeb67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 13:59:11.233409: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/jesper/Desktop/Neural-Network-python-from-scratch/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import keras.datasets.mnist as mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a418af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math class\n",
    "\n",
    "class Math:\n",
    "\n",
    "    # -----------------------------\n",
    "    # Activations\n",
    "    # -----------------------------\n",
    "    @staticmethod\n",
    "    def sigmoid(Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    @staticmethod\n",
    "    def deriv_sigmoid(Z):\n",
    "        s = Math.sigmoid(Z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    @staticmethod\n",
    "    def reLU(Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    @staticmethod\n",
    "    def deriv_reLU(Z):\n",
    "        return (Z > 0).astype(float)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(Z):\n",
    "        return np.tanh(Z)\n",
    "\n",
    "    @staticmethod\n",
    "    def deriv_tanh(Z):\n",
    "        return 1 - np.tanh(Z)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(Z):\n",
    "        expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "        return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(Z):\n",
    "        return Z\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # Loss functions\n",
    "    # -----------------------------\n",
    "    @staticmethod\n",
    "    def cross_entropy(Y_pred, Y_true):\n",
    "        m = Y_true.size\n",
    "        Y_one_hot = Math.one_hot(Y_true, Y_pred.shape[0])\n",
    "        return -np.sum(Y_one_hot * np.log(Y_pred + 1e-15)) / m\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_entropy_grad(Y_pred, Y_true):\n",
    "        m = Y_true.size\n",
    "        Y_one_hot = Math.one_hot(Y_true, Y_pred.shape[0])\n",
    "        return (Y_pred - Y_one_hot) / m\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(Y_pred, Y_true):\n",
    "        m = Y_true.size\n",
    "        Y_one_hot = Math.one_hot(Y_true, Y_pred.shape[0])\n",
    "        return np.sum((Y_pred - Y_one_hot)**2) / m\n",
    "\n",
    "    @staticmethod\n",
    "    def mse_grad(Y_pred, Y_true):\n",
    "        m = Y_true.size\n",
    "        Y_one_hot = Math.one_hot(Y_true, Y_pred.shape[0])\n",
    "        return 2 * (Y_pred - Y_one_hot) / m\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # One-hot encoding\n",
    "    # -----------------------------\n",
    "    @staticmethod\n",
    "    def one_hot(Y, num_classes):\n",
    "        one_hot_Y = np.zeros((num_classes, Y.size))\n",
    "        one_hot_Y[Y, np.arange(Y.size)] = 1\n",
    "        return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525775fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification NN class\n",
    "\n",
    "class ClassificationNN():\n",
    "    \n",
    "    # add more allowed activations and losses here if needed\n",
    "    _hidden_activations = ['reLU', 'sigmoid', 'tanh']\n",
    "    _output_activations = ['softmax', 'sigmoid', 'linear']\n",
    "    _losses = ['cross_entropy', 'mse']\n",
    "    \n",
    "    def __init__(self, layers, hidden_activation='reLU', output_activation='softmax', loss='cross_entropy'):\n",
    "        \n",
    "        self.layers = layers # List defining the number of layers and number of neurons in each layer\n",
    "        self.weights = [] # List to hold weights for each layer\n",
    "        self.biases = [] # List to hold biases for each layer\n",
    "        self.Zs = None # Linear combination per layer\n",
    "        self.As = None # Activation per layer\n",
    "        self.num_classes = layers[-1] # Number of output classes, used for one-hot encoding\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        for i in range(len(layers) - 1):\n",
    "            n_in = layers[i]\n",
    "            n_out = layers[i + 1]\n",
    "\n",
    "            W = np.random.randn(n_out, n_in) * np.sqrt(2 / n_in) # He initialization (good for ReLU, can be overridden in children classes)\n",
    "            b = np.zeros((n_out, 1))\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "        \n",
    "        # Hidden activation initialization\n",
    "        if hidden_activation not in self._hidden_activations:\n",
    "            raise ValueError(f\"Hidden activation must be one of {self._hidden_activations}\")\n",
    "        self.hidden_activation = hidden_activation\n",
    "\n",
    "        # Output activation initialization\n",
    "        if output_activation not in self._output_activations:\n",
    "            raise ValueError(f\"Output activation must be one of {self._output_activations}\")\n",
    "        self.output_activation = output_activation\n",
    "\n",
    "        # Loss initialization\n",
    "        if loss not in self._losses:\n",
    "            raise ValueError(f\"Loss must be one of {self._losses}\")\n",
    "        self.loss_name = loss\n",
    "        \n",
    "        \n",
    "    # Activation functions\n",
    "    def activation(self, Z, layer_index):\n",
    "        \"\"\"Returns activation for the layer\"\"\"\n",
    "        if layer_index == len(self.weights) - 1:\n",
    "            return getattr(Math, self.output_activation)(Z)\n",
    "        else:\n",
    "            return getattr(Math, self.hidden_activation)(Z)\n",
    "        \n",
    "    def activation_derivative(self, Z, layer_index):\n",
    "        \"\"\"Returns derivative for backpropagation\"\"\"\n",
    "        if layer_index == len(self.weights) - 1:\n",
    "            return np.ones_like(Z)  # Output handled in loss gradient\n",
    "        else:\n",
    "            return getattr(Math, f'deriv_{self.hidden_activation}')(Z).astype(float)\n",
    "        \n",
    "    \n",
    "    # Loss functions\n",
    "    def compute_loss(self, Y_pred, Y_true):\n",
    "        \"\"\"Compute scalar loss\"\"\"\n",
    "        return getattr(Math, self.loss_name)(Y_pred, Y_true)\n",
    "\n",
    "    def compute_loss_grad(self, Y_pred, Y_true):\n",
    "        \"\"\"Compute gradient of loss w.r.t output\"\"\"\n",
    "        return getattr(Math, f'{self.loss_name}_grad')(Y_pred, Y_true)\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.Zs = []\n",
    "        self.As = [X]\n",
    "        A = X\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            W, b = self.weights[i], self.biases[i]\n",
    "            Z = np.dot(W, A) + b\n",
    "            self.Zs.append(Z)\n",
    "\n",
    "            # Hidden layers use chosen hidden activation\n",
    "            if i < len(self.weights) - 1:\n",
    "                A = getattr(Math, self.hidden_activation)(Z)\n",
    "            else:\n",
    "                # Output layer uses chosen output activation\n",
    "                A = getattr(Math, self.output_activation)(Z)\n",
    "            self.As.append(A)\n",
    "\n",
    "        return A\n",
    "\n",
    "\n",
    "    def backward(self, Y):\n",
    "        m = Y.size\n",
    "        Y_pred = self.As[-1]\n",
    "        one_hot_Y = Math.one_hot(Y, self.num_classes)\n",
    "\n",
    "        # Compute output gradient\n",
    "        dZ = Y_pred - one_hot_Y\n",
    "\n",
    "        self.dWs = [None] * len(self.weights)\n",
    "        self.dbs = [None] * len(self.biases)\n",
    "\n",
    "        # Backpropagate through all layers\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            A_prev = self.As[i]\n",
    "            self.dWs[i] = np.dot(dZ, A_prev.T) / m\n",
    "            self.dbs[i] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "\n",
    "            if i > 0:\n",
    "                W = self.weights[i]\n",
    "                Z_prev = self.Zs[i-1]\n",
    "                dZ = np.dot(W.T, dZ) * getattr(Math, f'deriv_{self.hidden_activation}')(Z_prev).astype(float)\n",
    "\n",
    "\n",
    "    def update_params(self, dWs, dbs, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * dWs[i]\n",
    "            self.biases[i] -= learning_rate * dbs[i]\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        Y_pred = self.forward(X)\n",
    "        return np.argmax(Y_pred, axis=0)\n",
    "\n",
    "    def accuracy(self, Y_pred, Y_true):\n",
    "        return np.mean(Y_pred == Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdff7bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement custom train mehod\n",
    "\n",
    "def train(model, X, Y, epochs, learning_rate, batch_size):\n",
    "        \"\"\"\n",
    "        Training loop with mini-batches and SGD updates.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            permutation = np.random.permutation(X.shape[1])\n",
    "            X_shuffled = X[:, permutation]\n",
    "            Y_shuffled = Y[permutation]\n",
    "\n",
    "            for j in range(0, X.shape[1], batch_size):\n",
    "                X_batch = X_shuffled[:, j:j+batch_size]\n",
    "                Y_batch = Y_shuffled[j:j+batch_size]\n",
    "\n",
    "                model.forward(X_batch)\n",
    "                model.backward(Y_batch)\n",
    "                model.update_params(model.dWs, model.dbs, learning_rate)\n",
    "\n",
    "            # Evaluate accuracy after each epoch\n",
    "            Y_pred = model.predict(X)\n",
    "            acc = model.accuracy(Y_pred, Y)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7cc7da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Accuracy: 0.8911\n",
      "Epoch 2/10 - Accuracy: 0.9122\n",
      "Epoch 3/10 - Accuracy: 0.9225\n",
      "Epoch 4/10 - Accuracy: 0.9314\n",
      "Epoch 5/10 - Accuracy: 0.9375\n",
      "Epoch 6/10 - Accuracy: 0.9417\n",
      "Epoch 7/10 - Accuracy: 0.9448\n",
      "Epoch 8/10 - Accuracy: 0.9502\n",
      "Epoch 9/10 - Accuracy: 0.9531\n",
      "Epoch 10/10 - Accuracy: 0.9552\n",
      "Test Accuracy: 0.9521\n"
     ]
    }
   ],
   "source": [
    "# init data and use NN\n",
    "\n",
    "# load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Prepare training data\n",
    "m_train = X_train.shape[0]\n",
    "X_train_flat = X_train.reshape(m_train, -1).T / 255.0\n",
    "Y_train = y_train.astype(int)\n",
    "\n",
    "# Prepare test data (separate, unseen data)\n",
    "m_test = X_test.shape[0]\n",
    "X_test_flat = X_test.reshape(m_test, -1).T / 255.0\n",
    "Y_test = y_test.astype(int)\n",
    "\n",
    "model = ClassificationNN(\n",
    "    layers=[784, 128, 64, 10],\n",
    "    hidden_activation='reLU',\n",
    "    output_activation='softmax',\n",
    "    loss='cross_entropy'\n",
    ")\n",
    "\n",
    "# train and test model\n",
    "train(model, X_train_flat, Y_train, epochs=10, learning_rate=0.01, batch_size=64)\n",
    "Y_pred_test = model.predict(X_test_flat)\n",
    "accuracy = model.accuracy(Y_pred_test, Y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
