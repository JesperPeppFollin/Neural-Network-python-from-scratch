{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c5889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc16046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Math:\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def reLU(Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    @staticmethod\n",
    "    def deriv_reLU(Z):\n",
    "        return Z > 0\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(Z):\n",
    "        # Subtract max per sample for numerical stability\n",
    "        expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "        return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "\n",
    "    # detta Ã¤r knappast en math funktion men okej\n",
    "    @staticmethod\n",
    "    def one_hot(Y):\n",
    "        one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "        one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "        one_hot_Y = one_hot_Y.T\n",
    "        return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d4d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.As = None\n",
    "        self.Zs = None\n",
    "        \n",
    "        for i in range(len(layers) - 1):\n",
    "            n_in  = layers[i]\n",
    "            n_out = layers[i + 1]\n",
    "            \n",
    "            # He initialization (best for ReLU)\n",
    "            W = np.random.randn(n_out, n_in) * np.sqrt(2 / n_in)\n",
    "            b = np.zeros((n_out, 1))\n",
    "            \n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    \n",
    "    def forward_prop(self, X):\n",
    "        \n",
    "        # store activations so to avoid recalculate is needed in backdrop\n",
    "        self.Zs = []\n",
    "        self.As = [X]\n",
    "        A = X\n",
    "        \n",
    "        # all hidden layers with ReLU\n",
    "        for i in range(len(self.layers) - 2):\n",
    "            W = self.weights[i]\n",
    "            b = self.biases[i]\n",
    "            Z = np.dot(W, A) + b\n",
    "            A = Math.reLU(Z)\n",
    "            self.Zs.append(Z)\n",
    "            self.As.append(A)\n",
    "            \n",
    "        # Final output layer with softmax\n",
    "        W = self.weights[len(self.layers) - 2]\n",
    "        b = self.biases[len(self.layers) - 2]\n",
    "        Z = np.dot(W, A) + b\n",
    "        A = Math.softmax(Z)\n",
    "        self.Zs.append(Z)\n",
    "        self.As.append(A)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def back_prop(self, X, Y):\n",
    "        \n",
    "        # last output layer\n",
    "        m = Y.size\n",
    "        one_hot_Y = Math.one_hot(Y)\n",
    "        \n",
    "        # all hidden layers\n",
    "        for i in range(len(self.layers) - 2, -1, -1):\n",
    "            \n",
    "            W = self.weights[i]\n",
    "            b = self.biases[i]\n",
    "        \n",
    "            \n",
    "            dZ2 = A2 - one_hot_Y\n",
    "            dW2 = 1/m * dZ2.dot(A1.T)\n",
    "            db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "            \n",
    "            dZ1 = W2.T.dot(dZ2) * Math.deriv_reLU(Z1)\n",
    "            dW1 = 1/m * dZ1.dot(X.T)\n",
    "            db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        pass\n",
    "    \n",
    "    def train(self, X, Y, epochs, learning_rate):\n",
    "        for epochs in range(epochs):\n",
    "            self.forward_prop(X)\n",
    "            self.back_prop(X, Y)\n",
    "            self.update_params(learning_rate)\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e1ca145",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mnist.load_data()\n",
    "\n",
    "nn = NeuralNetwork([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
