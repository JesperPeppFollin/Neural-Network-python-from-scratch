{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e63a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist, fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466fadd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Math:\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def reLU(Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    @staticmethod\n",
    "    def deriv_reLU(Z):\n",
    "        return Z > 0\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(Z):\n",
    "        expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "        return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "\n",
    "    # detta Ã¤r knappt en math funktion men okej\n",
    "    @staticmethod\n",
    "    def one_hot(Y, nbr_classes):\n",
    "        one_hot_Y = np.zeros((Y.size, nbr_classes))\n",
    "        one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "        one_hot_Y = one_hot_Y.T\n",
    "        return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e96b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c30778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers): # ska ta in x antal Layer objekt och sedan omvandla till weights och biases\n",
    "        self.layers = layers\n",
    "    \n",
    "    \n",
    "    def forward_prop(self, X):\n",
    "        pass\n",
    "    \n",
    "    def back_prop(self, Y):\n",
    "        pass        \n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        pass\n",
    "            \n",
    "    def predict(self, X):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def accuracy(self, predictions, Y):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Prepare training data\n",
    "m_train = X_train.shape[0]\n",
    "X_train_flat = X_train.reshape(m_train, -1).T / 255.0\n",
    "Y_train = y_train.astype(int)\n",
    "\n",
    "# Prepare test data (separate, unseen data)\n",
    "m_test = X_test.shape[0]\n",
    "X_test_flat = X_test.reshape(m_test, -1).T / 255.0\n",
    "Y_test = y_test.astype(int)\n",
    "\n",
    "nn = NeuralNetwork(\n",
    "        Layer(784, 128, activation='reLU'), #eller typ Linear(784, 128, activation='reLU')\n",
    "        Layer(128, 64, activation='reLU'),\n",
    "        Layer(64, 10, activation='softmax')\n",
    ")\n",
    "\n",
    "# train and test model\n",
    "nn.train(X_train_flat, Y_train, epochs=10, learning_rate=0.01)\n",
    "predictions = nn.predict(X_test_flat)\n",
    "accuracy = nn.accuracy(predictions, Y_test)\n",
    "print(f\"Test Accuracy = {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
